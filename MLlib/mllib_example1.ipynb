{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"airbnb_example\") \\\n",
    "    .master(\"local\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/hyunseokjung/data/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet'\n",
    "airbnbDF = spark.read.format('parquet').load(file_path)\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
    "                \"number_of_reviews\", \"price\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/05 11:33:29 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "There are 5780 rows in the training set,\n",
      "      and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set,\n",
    "      and {testDF.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=['bedrooms'], outputCol='features')\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/05 11:44:17 WARN Instrumentation: [e6667f9f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "22/12/05 11:44:17 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "22/12/05 11:44:17 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/12/05 11:44:17 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/12/05 11:44:17 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is\n",
      "      price = 123.68*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "print(f'''The formula for the linear regression line is\n",
    "      price = {m}*bedrooms + {b}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/05 11:53:59 WARN Instrumentation: [9cb4e9a0] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# ã„·\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+------------------+\n",
      "|bedrooms|features|price|        prediction|\n",
      "+--------+--------+-----+------------------+\n",
      "|     1.0|   [1.0]| 85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|159.0|171.18598011578285|\n",
      "+--------+--------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('host_is_superhost', 'string'),\n",
       " ('cancellation_policy', 'string'),\n",
       " ('instant_bookable', 'string'),\n",
       " ('host_total_listings_count', 'double'),\n",
       " ('neighbourhood_cleansed', 'string'),\n",
       " ('latitude', 'double'),\n",
       " ('longitude', 'double'),\n",
       " ('property_type', 'string'),\n",
       " ('room_type', 'string'),\n",
       " ('accommodates', 'double'),\n",
       " ('bathrooms', 'double'),\n",
       " ('bedrooms', 'double'),\n",
       " ('beds', 'double'),\n",
       " ('bed_type', 'string'),\n",
       " ('minimum_nights', 'double'),\n",
       " ('number_of_reviews', 'double'),\n",
       " ('review_scores_rating', 'double'),\n",
       " ('review_scores_accuracy', 'double'),\n",
       " ('review_scores_cleanliness', 'double'),\n",
       " ('review_scores_checkin', 'double'),\n",
       " ('review_scores_communication', 'double'),\n",
       " ('review_scores_location', 'double'),\n",
       " ('review_scores_value', 'double'),\n",
       " ('price', 'double'),\n",
       " ('bedrooms_na', 'double'),\n",
       " ('bathrooms_na', 'double'),\n",
       " ('beds_na', 'double'),\n",
       " ('review_scores_rating_na', 'double'),\n",
       " ('review_scores_accuracy_na', 'double'),\n",
       " ('review_scores_cleanliness_na', 'double'),\n",
       " ('review_scores_checkin_na', 'double'),\n",
       " ('review_scores_communication_na', 'double'),\n",
       " ('review_scores_location_na', 'double'),\n",
       " ('review_scores_value_na', 'double')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes\n",
    "                   if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                              outputCols=indexOutputCols,\n",
    "                              handleInvalid='skip')\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes\n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assembleInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assembleInputs,\n",
    "                               outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_7104e1c374cd"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "rFormula = RFormula(formula = \"price ~ .\",\n",
    "                    featuresCol = \"features\",\n",
    "                    labelCol = \"price\",\n",
    "                    handleInvalid = \"skip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [rFormula, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/05 12:52:10 WARN Instrumentation: [9506692d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "22/12/05 12:52:10 WARN Instrumentation: [9506692d] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,7,23,4...| 85.0| 55.30094763354373|\n",
      "|(98,[0,3,6,7,23,4...| 45.0| 22.70940291742818|\n",
      "|(98,[0,3,6,7,23,4...| 70.0|27.182906571761578|\n",
      "|(98,[0,3,6,7,13,4...|128.0|-91.90969569190747|\n",
      "|(98,[0,3,6,7,13,4...|159.0| 94.54162775821169|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol = \"prediction\",\n",
    "    labelCol = \"price\",\n",
    "    metricName = \"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.15985154393435386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/05 15:16:16 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 437020 ms exceeds timeout 120000 ms\n",
      "22/12/05 15:16:16 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbc2b592ccd6079e95c9c6b0edf643b0189dd6a289bfe01be45caefb75409e7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
