{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row, SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('recommendation') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/hyunseokjung/data/spark_guide'\n",
    "\n",
    "ratings = spark.read.text(f\"{data_path}/sample_movielens_ratings.txt\") \\\n",
    "    .rdd.toDF() \\\n",
    "    .selectExpr(\"split(value, '::') as col\") \\\n",
    "    .selectExpr(\n",
    "        \"cast(col[0] as int) as userId\",\n",
    "        \"cast(col[1] as int) as movieId\",\n",
    "        \"cast(col[2] as float) as rating\",\n",
    "        \"cast(col[3] as long) as timestamp\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS() \\\n",
    "    .setMaxIter(5) \\\n",
    "    .setRegParam(0.01) \\\n",
    "    .setUserCol(\"userId\") \\\n",
    "    .setItemCol(\"movieId\") \\\n",
    "    .setRatingCol(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: alpha for implicit preference (default: 1.0)\n",
      "blockSize: block size for stacking input data in matrices. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. (default: 4096)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "coldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: 'nan', 'drop'. (default: nan)\n",
      "finalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\n",
      "implicitPrefs: whether to use implicit preference (default: False)\n",
      "intermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: movieId)\n",
      "maxIter: max number of iterations (>= 0). (default: 10, current: 5)\n",
      "nonnegative: whether to use nonnegative constraint for least squares (default: False)\n",
      "numItemBlocks: number of item blocks (default: 10)\n",
      "numUserBlocks: number of user blocks (default: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "rank: rank of the factorization (default: 10)\n",
      "ratingCol: column name for ratings (default: rating, current: rating)\n",
      "regParam: regularization parameter (>= 0). (default: 0.1, current: 0.01)\n",
      "seed: random seed. (default: 1897853915470281180)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (default: user, current: userId)\n"
     ]
    }
   ],
   "source": [
    "print(als.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "alsModel = als.fit(train)\n",
    "predictions = alsModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "|     0|      3|   1.0|1424380312| 0.6339553|\n",
      "|     0|      5|   2.0|1424380312| 1.4654416|\n",
      "|     0|     21|   1.0|1424380312| 1.6357095|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|userId|            col|\n",
      "+------+---------------+\n",
      "|    20|{22, 4.6268606}|\n",
      "|    20| {75, 4.272371}|\n",
      "|    20|{54, 4.1329565}|\n",
      "|    10| {38, 4.204379}|\n",
      "|    10|{46, 3.8890886}|\n",
      "|    10| {74, 3.536286}|\n",
      "+------+---------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 624:===========================================>          (80 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|movieId|            col|\n",
      "+-------+---------------+\n",
      "|     20|{26, 4.9450316}|\n",
      "|     20| {17, 4.641791}|\n",
      "|     20| {2, 4.1928797}|\n",
      "|     40|  {6, 4.387648}|\n",
      "|     40| {2, 3.8655288}|\n",
      "|     40| {7, 3.4234028}|\n",
      "+-------+---------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "alsModel.recommendForAllUsers(3) \\\n",
    "    .selectExpr(\"userId\", \"explode(recommendations)\") \\\n",
    "    .show(6)\n",
    "alsModel.recommendForAllItems(3) \\\n",
    "    .selectExpr(\"movieId\", \"explode(recommendations)\") \\\n",
    "    .show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 2.019469736413673\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator() \\\n",
    "    .setMetricName(\"rmse\") \\\n",
    "    .setLabelCol(\"rating\") \\\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root-mean-square error = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "regComparison = predictions.select(\"rating\", \"prediction\") \\\n",
    "    .rdd.map(lambda x: (x(0), x(1)))\n",
    "metrics = RegressionMetrics(regComparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 13:54:15 ERROR Executor: Exception in task 0.0 in stage 715.0 (TID 1842)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/session.py\", line 910, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1700, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1716, in verify_default\n",
      "    verify_acceptable_types(obj)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n",
      "    raise TypeError(\n",
      "TypeError: field prediction: DoubleType() can not accept object Row(1.0=0) in type <class 'pyspark.sql.types.Row'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "22/12/02 13:54:15 WARN TaskSetManager: Lost task 0.0 in stage 715.0 (TID 1842) (ingu627 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/session.py\", line 910, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1700, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1716, in verify_default\n",
      "    verify_acceptable_types(obj)\n",
      "  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n",
      "    raise TypeError(\n",
      "TypeError: field prediction: DoubleType() can not accept object Row(1.0=0) in type <class 'pyspark.sql.types.Row'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "\n",
      "22/12/02 13:54:15 ERROR TaskSetManager: Task 0 in stage 715.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1333.rootMeanSquaredError.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 715.0 failed 1 times, most recent failure: Lost task 0.0 in stage 715.0 (TID 1842) (ingu627 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/session.py\", line 910, in prepare\n    verify_func(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1700, in verify_struct\n    verifier(v)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1716, in verify_default\n    verify_acceptable_types(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n    raise TypeError(\nTypeError: field prediction: DoubleType() can not accept object Row(1.0=0) in type <class 'pyspark.sql.types.Row'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:58)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:70)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:62)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:106)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:115)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/session.py\", line 910, in prepare\n    verify_func(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1700, in verify_struct\n    verifier(v)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1716, in verify_default\n    verify_acceptable_types(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n    raise TypeError(\nTypeError: field prediction: DoubleType() can not accept object Row(1.0=0) in type <class 'pyspark.sql.types.Row'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/hyunseokjung/Github/BigData/spark/spark_recommender.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hyunseokjung/Github/BigData/spark/spark_recommender.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m metrics\u001b[39m.\u001b[39;49mrootMeanSquaredError\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/mllib/evaluation.py:198\u001b[0m, in \u001b[0;36mRegressionMetrics.rootMeanSquaredError\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m@property\u001b[39m  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m1.4.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrootMeanSquaredError\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m    194\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m    Returns the root mean squared error, which is defined as the square root of\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m    the mean squared error.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(\u001b[39m\"\u001b[39;49m\u001b[39mrootMeanSquaredError\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/mllib/common.py:157\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39ma: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m callJavaFunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sc, \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_model, name), \u001b[39m*\u001b[39;49ma)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/mllib/common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, func(\u001b[39m*\u001b[39;49mjava_args))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1333.rootMeanSquaredError.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 715.0 failed 1 times, most recent failure: Lost task 0.0 in stage 715.0 (TID 1842) (ingu627 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/session.py\", line 910, in prepare\n    verify_func(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1700, in verify_struct\n    verifier(v)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1716, in verify_default\n    verify_acceptable_types(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n    raise TypeError(\nTypeError: field prediction: DoubleType() can not accept object Row(1.0=0) in type <class 'pyspark.sql.types.Row'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:58)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:70)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:62)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:74)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:106)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:115)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/hyunseokjung/spark-3.3.0/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/session.py\", line 910, in prepare\n    verify_func(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1700, in verify_struct\n    verifier(v)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1716, in verify_default\n    verify_acceptable_types(obj)\n  File \"/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n    raise TypeError(\nTypeError: field prediction: DoubleType() can not accept object Row(1.0=0) in type <class 'pyspark.sql.types.Row'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics, RegressionMetrics\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "perUserActual = predictions \\\n",
    "    .where(\"rating > 2.5\") \\\n",
    "    .groupBy(\"userId\") \\\n",
    "    .agg(expr(\"collect_set(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "perUserPredictions = predictions \\\n",
    "    .orderBy(col(\"userId\"), expr(\"prediction DESC\")) \\\n",
    "    .groupBy(\"userId\") \\\n",
    "    .agg(expr(\"collect_list(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyunseokjung/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "perUserActualvPred = perUserActual.join(perUserPredictions, [\"userId\"]).rdd \\\n",
    "    .map(lambda row: (row[1], row[2][:15]))\n",
    "ranks = RankingMetrics(perUserActualvPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5076923076923078"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.meanAveragePrecision\n",
    "ranks.precisionAt(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
